# -*- coding: utf-8 -*-
"""nlp-tokenization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EoYCH-0aMCQzmASHuWflUxPulRuGadTb
"""

text1 = "I am going to Delhi tomorrow. I hope to see you there as well."
text2 = "When are you coming here? I am worried sick."

# technique 1
# split function

# sentence level tokenization
sentence_tokens = text1.split('.')
print(sentence_tokens)

# word level tokenization
word_tokens = text1.split(' ')
print(word_tokens)

# problem

print(text2.split('.'))
print(text2.split(' '))

# as we can see the punctuations are coming along with the words and not seperate
# hence split function cannot handle prefices, infices and suffices

# technique 2
# nltk library

import nltk
sentence_tokens = nltk.sent_tokenize(text1)
print(sentence_tokens)

word_tokens = nltk.word_tokenize(text1)
print(word_tokens)

sentence_tokens = nltk.sent_tokenize(text2)
print(sentence_tokens)

word_tokens = nltk.word_tokenize(text2)
print(word_tokens)

# technique 3
# spacy library

import spacy
nlp = spacy.load('en_core_web_sm')

doc1 = nlp(text1)
doc2 = nlp(text2)

for doc in doc1:
  print(doc)

print("\n")

for doc in doc2:
  print(doc)

