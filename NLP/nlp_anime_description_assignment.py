# -*- coding: utf-8 -*-
"""nlp-anime-description-assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ThUvLdq40N6icyPa3mFJ96R46dpeoCyy
"""

nltk.download('punkt_tab')
nltk.download('wordnet')
nltk.download('stopwords')

import pandas as pd
import numpy as np
import emoji
import nltk
import re
import csv
import spacy
import string
from textblob import TextBlob
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk import word_tokenize, sent_tokenize

import requests

url = "https://imdb.iamidiotareyoutoo.com/search?q=Iron-Man"

response = requests.get(url)

response_json = response.json()
response_json_desc = response_json['description'][0]
response_json_desc

!pip install jikanpy-v4

import requests
from jikanpy import Jikan

session = requests.Session()
# Set custom persistent headers that will be used with all HTTP requests with your session
session.headers.update({'x-test': 'true'})

jikan = Jikan(session=session)

mushishi = jikan.anime(20)
# data = mushishi['data']
# data['title']
mushishi

myAnimeInfo = dict()
count = 0
for i in range(20,23):
  mushishi = jikan.anime(i)
  animeData = mushishi['data']
  title = animeData['title']
  epsNum = animeData['episodes']
  status = animeData['status']
  info = animeData['synopsis']
  genreData = animeData['genres']
  genres = []
  for j in range(len(genreData)):
    genres.append(genreData[j]['name'])
  myAnimeInfo[count] = {
      'title':title,
      'epsNum':epsNum,
      'status':status,
      'synopsys':info,
      'genres':genres
  }
  count+=1

for i in range(23,26):
  mushishi = jikan.anime(i)
  animeData = mushishi['data']
  title = animeData['title']
  epsNum = animeData['episodes']
  status = animeData['status']
  info = animeData['synopsis']
  genreData = animeData['genres']
  genres = []
  for j in range(len(genreData)):
    genres.append(genreData[j]['name'])
  myAnimeInfo[count] = {
      'title':title,
      'epsNum':epsNum,
      'status':status,
      'synopsys':info,
      'genres':genres
  }
  count+=1

for i in range(26,29):
  mushishi = jikan.anime(i)
  animeData = mushishi['data']
  title = animeData['title']
  epsNum = animeData['episodes']
  status = animeData['status']
  info = animeData['synopsis']
  genreData = animeData['genres']
  genres = []
  for j in range(len(genreData)):
    genres.append(genreData[j]['name'])
  myAnimeInfo[count] = {
      'title':title,
      'epsNum':epsNum,
      'status':status,
      'synopsys':info,
      'genres':genres
  }
  count+=1

for i in range(29,32):
  mushishi = jikan.anime(i)
  animeData = mushishi['data']
  title = animeData['title']
  epsNum = animeData['episodes']
  status = animeData['status']
  info = animeData['synopsis']
  genreData = animeData['genres']
  genres = []
  for j in range(len(genreData)):
    genres.append(genreData[j]['name'])
  myAnimeInfo[count] = {
      'title':title,
      'epsNum':epsNum,
      'status':status,
      'synopsys':info,
      'genres':genres
  }
  count+=1

idList = [52991, 5114, 9253, 38524, 28977, 39486, 60022, 11061, 9969, 15417, 820, 41467, 34096, 43608, 42938, 60489, 60489, 4181, 918, 28851]
for i in idList[0:3]:
  mushishi = jikan.anime(i)
  animeData = mushishi['data']
  title = animeData['title']
  epsNum = animeData['episodes']
  status = animeData['status']
  info = animeData['synopsis']
  genreData = animeData['genres']
  genres = []
  for j in range(len(genreData)):
    genres.append(genreData[j]['name'])
  myAnimeInfo[count] = {
      'title':title,
      'epsNum':epsNum,
      'status':status,
      'synopsys':info,
      'genres':genres
  }
  count+=1

for i in idList[3:6]:
  mushishi = jikan.anime(i)
  animeData = mushishi['data']
  title = animeData['title']
  epsNum = animeData['episodes']
  status = animeData['status']
  info = animeData['synopsis']
  genreData = animeData['genres']
  genres = []
  for j in range(len(genreData)):
    genres.append(genreData[j]['name'])
  myAnimeInfo[count] = {
      'title':title,
      'epsNum':epsNum,
      'status':status,
      'synopsys':info,
      'genres':genres
  }
  count+=1

for i in idList[6:9]:
  mushishi = jikan.anime(i)
  animeData = mushishi['data']
  title = animeData['title']
  epsNum = animeData['episodes']
  status = animeData['status']
  info = animeData['synopsis']
  genreData = animeData['genres']
  genres = []
  for j in range(len(genreData)):
    genres.append(genreData[j]['name'])
  myAnimeInfo[count] = {
      'title':title,
      'epsNum':epsNum,
      'status':status,
      'synopsys':info,
      'genres':genres
  }
  count+=1

for i in idList[9:12]:
  mushishi = jikan.anime(i)
  animeData = mushishi['data']
  title = animeData['title']
  epsNum = animeData['episodes']
  status = animeData['status']
  info = animeData['synopsis']
  genreData = animeData['genres']
  genres = []
  for j in range(len(genreData)):
    genres.append(genreData[j]['name'])
  myAnimeInfo[count] = {
      'title':title,
      'epsNum':epsNum,
      'status':status,
      'synopsys':info,
      'genres':genres
  }
  count+=1

for i in idList[12:15]:
  mushishi = jikan.anime(i)
  animeData = mushishi['data']
  title = animeData['title']
  epsNum = animeData['episodes']
  status = animeData['status']
  info = animeData['synopsis']
  genreData = animeData['genres']
  genres = []
  for j in range(len(genreData)):
    genres.append(genreData[j]['name'])
  myAnimeInfo[count] = {
      'title':title,
      'epsNum':epsNum,
      'status':status,
      'synopsys':info,
      'genres':genres
  }
  count+=1

database = pd.DataFrame.from_dict(myAnimeInfo,orient='index')
database

def lowercase(text):
  return text.lower()

def remove_html(text):
  pattern = re.compile('<.*?>')
  return pattern.sub(r' ',text)

def remove_url(text):
  pattern = re.compile(r'https?://\S+|www\.\S+')
  return pattern.sub(r'',text)

def remove_punctuations(text):
  all_puncs = string.punctuation
  return text.translate(str.maketrans('','',all_puncs))

def remove_slangs(text):
  with open("/content/slang.txt","r") as f:
    all_slangs_file = csv.reader(f,delimiter="=")
    all_slangs = list(all_slangs_file)
  all_slangs_dict = dict(all_slangs)

  new_text_list = []
  words = text.split()
  for word in words:
    if word.upper() in all_slangs_dict.keys():
      new_text_list.append(all_slangs_dict[word.upper()])
    else:
      new_text_list.append(word)
  new_text = ' '.join(new_text_list)
  return new_text

def spell_correction(text):
  textBlob = TextBlob(text)
  return textBlob.correct().string

def remove_stopwords(text):
  en_stopwords = stopwords.words('english')
  new_text= []
  words = text.split(' ')
  for word in words:
    if word.lower() not in en_stopwords:
      new_text.append(word)
  return ' '.join(new_text)

def remove_emojis(text):
  return emoji.demojize(text)

def word_token_func(text):
  return word_tokenize(text)

def stemming(word_tokens):
  ps = PorterStemmer()
  word_roots = []
  for word in word_tokens:
    word_roots.append(ps.stem(word))
  return ' '.join(word_roots)

def lemmatize(word_tokens):
  lemmatizer = WordNetLemmatizer()
  word_roots = []
  for word in word_tokens:
    word_roots.append(lemmatizer.lemmatize(word,pos='v'))
  return ' '.join(word_roots)

def text_preprocessing(text):
  text = lowercase(text)
  text = remove_html(text)
  text = remove_url(text)
  text = remove_punctuations(text)
  text = spell_correction(text)
  text = remove_stopwords(text)
  text = remove_emojis(text)
  word_tokens = word_token_func(text)
  # rooted_tokens = stemming(word_tokens)
  rooted_tokens = lemmatize(word_tokens)
  return rooted_tokens

database['modified_synopsys'] = database['synopsys'].apply(text_preprocessing)
database

database['modified_synopsys']

