# -*- coding: utf-8 -*-
"""nlp-text-representation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1snCRcRIUHvVGax2xc-9phTNd49hDRo00
"""

# !pip install nltk spacy gensim

import nltk
import spacy
import re
import numpy as np

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

corpus = '''
Monkey D. Luffy is the captain of the Straw Hat Pirates, and dreamt of being a pirate since childhood from the influence of his idol and mentor Red-Haired Shanks. At the age of 17, Luffy sets sail from the East Blue Sea to the Grand Line in search of the legendary treasure One Piece, to succeed Gol D. Roger as "King of the Pirates". He fights multiple antagonists, and aids and befriends the inhabitants of several islands on his journey. Usually cheerful, he becomes serious and even aggressive when he fights. Luffy uses his rubber body to concentrate his power, executing a range of attacks. In his signature attack, the Gum-Gum Pistol, he slingshots punches at opponents from a distance. Luffy also grows stronger over the course of the story by transforming his body through different "Gears"; this is reflected in his bounty, which is used to measure the threat he poses to the World Government, which forbids piracy. Luffy clashes with the three kinds of great powers in One Piece: the World Government's Marines and its allied privateers known as the Seven Warlords of the Sea, and the most influential pirate captains known as the Four Emperors.
'''

# preprocessing

# lowercasing
corpus = corpus.lower()

# tokenization
tokenized_words = nltk.word_tokenize(corpus)
print(tokenized_words)

# removing stopwords
from nltk.corpus import stopwords
stopwords_list = stopwords.words('english')
non_stopwords_corpus = []
for word in tokenized_words:
  if word not in stopwords_list:
    non_stopwords_corpus.append(word)
print(non_stopwords_corpus)

# stemming
from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()
stemmed_corpus = []
for word in non_stopwords_corpus:
  stemmed_corpus.append(stemmer.stem(word))
print(stemmed_corpus)

# lemmatization
from nltk.stem import WordNetLemmatizer
lemma = WordNetLemmatizer()
lemmatized_words = []
for word in non_stopwords_corpus:
  lemmatized_words.append(lemma.lemmatize(word,pos='v'))
print(lemmatized_words)

# One Hot Encoding
size = len(lemmatized_words)
vocabulary = np.zeros((size,size),dtype='int32')

idxi = 0
for idxi, word_row in enumerate(lemmatized_words):
  for idxj, word_col in enumerate(lemmatized_words):
    if word_row == word_col:
      vocabulary[idxi][idxj] = 1
print(vocabulary)

word_to_index = {}
count = 0
for word in lemmatized_words:
  word_to_index.update({word:count})
  count+=1

### testing
sample_sent = 'Luffy set sail in search of the treasure'

# lowercasing
sample_sent = sample_sent.lower()

# tokenize test statement
tokenized_test = nltk.word_tokenize(sample_sent)

# remove stopwords
non_stopwords_test = []
for word in tokenized_test:
  if word not in stopwords.words('english'):
    non_stopwords_test.append(word)
print(non_stopwords_test)

ohe_form = []
for word in non_stopwords_test:
  if word not in word_to_index:
    raise ValueError('Out of Vocabulary for given test sentence')
  else:
    ohe_form.append(vocabulary[word_to_index[word]])

print(ohe_form)

# Bag of words

from sklearn.feature_extraction.text import CountVectorizer

bow = CountVectorizer()

bow_vocabulary = bow.fit_transform(lemmatized_words)

print(bow.vocabulary_)

print(bow_vocabulary)

# Tf-IDF

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()

tfidf_vocabulary = tfidf.fit_transform(lemmatized_words).toarray()

print(tfidf.vocabulary_)

print(tfidf_vocabulary)

# NGrams
from sklearn.feature_extraction.text import CountVectorizer

ngram = CountVectorizer(ngram_range=(3,3))

# for ngrams, it expects list of single string instead of tokenized list


ngram_vocabulary_mapping = ngram.fit_transform([' '.join(lemmatized_words)])

print(ngram.vocabulary_)

print(ngram_vocabulary_mapping)

# word2vec - pretrained model
from gensim.models import Word2Vec, KeyedVectors

import gensim.downloader as api

wv = api.load('word2vec-google-news-300')

print(wv['king'])

print(wv.most_similar('man'))
print(wv.doesnt_match(['king','prince','box']))
print(wv.similarity('king','man'))