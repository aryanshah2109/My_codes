# -*- coding: utf-8 -*-
"""NLP-text-preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZuKe_5vZVkfK_tQ1bjEM9-Le8Xuv6n1B
"""

import pandas as pd
import re
import string
import csv
import nltk
from textblob import TextBlob

import kagglehub

# Download latest version
path = kagglehub.dataset_download("lakshmi25npathi/imdb-dataset-of-50k-movie-reviews")

print("Path to dataset files:", path)

data = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')

data

mystring = data['review'][3]

# Lowercasing
mystring_lowercased = mystring.lower()
print(mystring)
print(mystring_lowercased)

# applying to entire dataset
def to_lower(s):
  return s.lower()
data['review'] = data['review'].apply(to_lower)
data

# removing html tags by regex
def remove_html(text):
  pattern = re.compile('<.*?>')
  return pattern.sub(r' ',text)
mystring_html_removed = remove_html(mystring_lowercased)
print(mystring_html_removed)

# applying to entire dataset
data['review'] = data['review'].apply(remove_html)
data

# removing urls
mystr = 'For more information, check out https://www.wikipedia.com . You can also search on www.google.com'
def remove_url(text):
  pattern = re.compile(r'https?://\S+|www\.\S+')
  return pattern.sub(r'',text)

mystr_no_url = remove_url(mystr)
mystr_no_url

# apply on entire dataset
data['review'] = data['review'].apply(remove_url)
data

# removing punctuations
all_punctuations = string.punctuation
def remove_punc(text):
  return text.translate(str.maketrans('','',all_punctuations))
print(data['review'][3])
print(remove_punc(data['review'][3]))

# for entire dataset
data['review'] = data['review'].apply(remove_punc)
data['review']

# chat words removal
all_slangs = []
# reading all slangs and their full forms
with open(r"D:\CODING_CODES\AIML\NLP\slang.txt","r") as f:
  all_slangs_file = csv.reader(f,delimiter="=")
  all_slangs = list(all_slangs_file)

all_slangs_dict = dict(all_slangs)

def remove_slangs(text):
  new_text_list = []
  words = text.split()
  for word in words:
    if word.upper() in all_slangs_dict.keys():
      new_text_list.append(all_slangs_dict[word.upper()])
    else:
      new_text_list.append(word)
  new_text = ' '.join(new_text_list)
  return new_text

print(remove_slangs("Hey I will call you asap as I am AFK"))

# apply to entire dataset
data['review'] = data['review'].apply(remove_slangs)
data['review']

# spelling correction


def rectify_spelling(text):
  textBlobObject = TextBlob(text)
  corrected_text = textBlobObject.correct().string
  return corrected_text

text = 'I aam curentlyy woorking aas a softwware enginneer'
print(rectify_spelling(text))

# apply to entire dataset
data['review'] = data['review'].apply(remove_slangs)
data['review']

# removing stop words
from nltk.corpus import stopwords

en_stopwords = stopwords.words('english')

def remove_stopwords(text):
  new_text = []
  all_words = text.split(' ')
  for word in all_words:
    if word not in en_stopwords:
      new_text.append(word)
  no_stopwords_text = ' '.join(new_text)
  return no_stopwords_text
print(remove_stopwords('She is a catholic taught in parochial elementary'))

# apply to entire dataset
data['review'] = data['review'].apply(remove_stopwords)
data['review']



# handling emojis

import emoji
emoji.demojize("Python is ðŸ”¥ ðŸ˜Ž")

